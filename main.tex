\documentclass[11pt, english]{article}
\usepackage{graphicx}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}
\usepackage[english]{babel}
\selectlanguage{english}
\usepackage[utf8]{inputenc}
\usepackage[svgnames]{xcolor}
\usepackage{amsmath}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator*{\E}{\mathbb{E}}



\usepackage{listings}
\usepackage{afterpage}
\pagestyle{plain}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

%\lstset{language=R,
%    basicstyle=\small\ttfamily,
%   stringstyle=\color{DarkGreen},
%    otherkeywords={0,1,2,3,4,5,6,7,8,9},
%    morekeywords={TRUE,FALSE},
%    deletekeywords={data,frame,length,as,character},
%    keywordstyle=\color{blue},
%    commentstyle=\color{DarkGreen},
%}

\lstset{frame=tb,
language=R,
aboveskip=3mm,
belowskip=3mm,
showstringspaces=false,
columns=flexible,
numbers=none,
keywordstyle=\color{blue},
numberstyle=\tiny\color{gray},
commentstyle=\color{dkgreen},
stringstyle=\color{mauve},
breaklines=true,
breakatwhitespace=true,
tabsize=3
}

\usepackage{here}


\textheight=21cm
\textwidth=17cm
%\topmargin=-1cm
\oddsidemargin=0cm
\parindent=0mm
\pagestyle{plain}

%%%%%%%%%%%%%%%%%%%%%%%%%%
% La siguiente instrucciÃ³n pone el curso automÃ¡ticamente%
%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{color}
\usepackage{ragged2e}

\global\let\date\relax
\newcounter{unomenos}
\setcounter{unomenos}{\number\year}
\addtocounter{unomenos}{-1}
\stepcounter{unomenos}
\gdef\@date{ Course \arabic{unomenos}/ 2019}

\begin{document}

\begin{titlepage}

\begin{center}
\vspace*{-1in}
\begin{figure}[htb]
\begin{center}
\includegraphics[width=8cm]{logo}
\end{center}
\end{figure}

INFORMATION \& NETWORK DYNAMICS (INDY) LAB - \@date\\
\vspace*{0.4in}
\begin{large}
MATRIX FACTORISATION\\
\end{large}
\vspace*{0.2in}
\begin{Large}
\textbf{Optimal Regularizer} \\
\end{Large}
\vspace*{0.3in}
\begin{large}
Ritabrata Ray \\
\end{large}
\vspace*{0.3in}
\rule{80mm}{0.1mm}\\
\vspace*{0.1in}
\begin{large}
IIT Kharapur \\
\end{large}
\includegraphics[width=2cm]{LogoFac.jpg}
\end{center}
\end{titlepage}

\newcommand{\CC}{C\nolinebreak\hspace{-.05em}\raisebox{.4ex}{\tiny\bf +}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf +}}
\def\CC{{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}}

\tableofcontents
\newpage
\section{Objective}
In this work, we aim to find the optimal values of the regularization strength of different parameters with different frequencies for the famous Matrix Completion Problem for the Netflix Problem. The parameters here are the regularization strengths of each embedding vector for each user and each movie.  
%Our Optimisation problem is as follows : \\
%\begin{center}
%\min\limits_{x \in R^d} f(x) & = \[ \frac{1}{n} %\sum\limits_{i=1}^{n} f \limits_{i}(x) \]
%\end{center} 
%\\ Convex function definition & L-lipchitz smooth  
\section{Matrix Factorization for the Netflix Problem}
The Netflix Problem involves a matrix $R$ of dimension $m \times n$ corresponding to $m$ users and $n$ items(movies). The $(i,j)^{th}$ entry of $R$ is a non-negative real number in $[0,5]$ is a rating of the $j^{th}$ item by the $i^{th}$ user. But the matrix $R$ is incomplete and on the basis of the available entries, we wish to predict the unavailable ones. We use latent factor model for this problem, which characterises each user and each item by a k-length \textit{embedding} vector. Where the inner product between the embedding vectors of the $i^{th}$ user and the $j^{th}$ item would be an approximation for the $(i,j)^th$ entry of $R$. We approximate $R$ as
\begin{equation}
    R\approx \hat{R}:=PQ 
\end{equation}
where $P$ is a $m \times k$ matix with each row being the embedding vector for each user and $Q$ is a $k \times n$ matrix with each column being the embedding vector for each item.
Learning the matrices $P$ and $Q$ from the observed data would make us predict $\hat{R}$ which is a good approximation to R.\\
Thus, we have
\begin{equation}
    \hat{r}_{ij}=\vec{p_{i}}^{T}\vec{q_{j}}=\sum\limits_{k=1}^{K}p_{ik}q_{kj}
\end{equation} 
Now, we attempt to learn the matrices $P$ and $Q$ by minimizing the RMSE(root mean square error) between $R$ and $\hat{R}$ at the available entries of $R$, using Stochastic Gradient Descent. As the problem involves a large number of parameters to be learnt, $mk+kn$ to be precise, in order to avoid overfitting we use regularization by including the Frobenius norm of the two unknown matrices $P$ and $Q$ in the cost function.\\\\
Let $T$ be the set of available entries in the matrix i.e.,
\begin{equation}
    T:=\{(i,j)|r_{ij}~is~available~in~R\}
\end{equation}
 %So, the total cost function is:
% \begin{equation}
    % L:=\sum\limits_{(i,j)\in T} (r_{ij}-\vec{p_{i}}\cdot 5\vec{q_{j}})^{2} +\norm{P}^2+\norm{Q}^2
 %\end{equation}
 If we uniformly sample an entry $(i,j)$ from $T$, the instantaneous cost function due to this entry would be:
 \begin{equation}
     \hat{l}=(r_{ij}-\vec{p_{i}}\cdot\vec{q_{j}})^2 + \norm{\vec{p_{i}}}^2+\norm{\vec{q_{j}}}^2
 \end{equation}
 where the norms are the $L_2$ norms of the vectors and we have the net cost function as:
 
 \begin{equation}
    L:= \mathbf{E}[\hat{l}]=\frac{1}{|T|}\sum\limits_{(i,j)\in T}(r_{ij}-\vec{p_{i}}\cdot\vec{q_{j}})^2+\frac{1}{|T|}\sum\limits_{i=1}^{m} s_i\norm{\vec{p_i}}^2+\frac{1}{|T|}\sum\limits_{j=1}^{n} t_j\norm{\vec{q_{j}}}^2
 \end{equation}
 where
 \begin{align}
      s_i &:=  |\{j|r_{ij}\in T\}| \\
      t_j  &:=  |\{i|r_{ij} \in T\}|
 \end{align}
 and 
 \begin{align}
     \sum\limits_{i=1}^{m} s_i &= |T|  \\
     \sum\limits_{j=1}^{n} t_j &= |T|
 \end{align}

\section{Knowledge Base Completion using Tensor Decomposition}
In knowledge base completion, the learner is given triples (subject, predicate, object) of facts about the world, and has to infer new triples that are likely but not yet known to be true. The standard completion task is link prediction, which consists in answering queries(subject, predicate, ?) or(?, predicate, object).\\
Relational data can be represented as a $\{0,1\}-$valued third order tensor $\mathbb{Y} \in \{0,1\}^{N \times P \times N}$, where $N$ is the total number of \textit{entities} and $P$ the number of \textit{predicates}, with $\mathbb{Y}_{i,j,k}=1$ if the relation $(i,j,k)$ is known. Tensor factorization algorithms can thus be used to infer a predicted tensor $\hat{\mathbb{X}} \in \mathbf{R}^{N \times P \times N}$ that approximates $\mathbb{Y}$ in a sense that we describe in the next subsection. Validation/test queries (?,j,k) are answered by ordering entities $i^'$ by decreasing values of $\hat{\mathbb{X}}_{i^',j,k}$, whereas queries (i,j,?) are answered by ordering entities $k^'$ by decreasing values of $\hat{\mathbb{X}}_{i,j,k^'}$.\\
The canonical decomposition of tensors, represents a tensor $\mathbb{X} \in \mathbf{R}^{N_1 \times N_2 \times N_3}$ as a sum of $R$ rank one tensors $u_{r}^{(1)} \otimes u_{r}^{(2)} \otimes u_{r}^{(3)}$ (with $\otimes$ the tensor product) where $r \in \{1, \ldots , R\}$, and $u_{r}^{(m)} \in \mathbf{R}^{N_m}$:
\begin{equation}
    \mathbb{X}=\sum \limits_{r=1}^{R} u_{r}^{(1)} \otimes u_{r}^{(2)} \otimes u_{r}^{(3)}
\end{equation}
Given $\mathbb{X}$, the smallest $R$ for which this decomposition holds is called the canonical rank of $\mathbb{X}$.\\
Two popular models are:\\
\textbf{DistMult.} It represents a tensor $\mathbb{X} \in \mathbf{R}^{N \times P \times N}$ as a sum of rank-1 tensors $u_{r}^{(1)} \otimes u_{r}^{(2)} \otimes u_{r}^{(1)}$:
\begin{equation}
    \mathbb{X}=\sum\limits_{r=1}^{R} u_{r}^{(1)} \otimes u_{r}^{(2)} \otimes u_{r}^{(1)}
\end{equation}

\textbf{ComplEx.}Dist-Mult yields a tensor that is symmetric in the object and subject modes. The assumption that the data tensor can be properly approximated by a symmetric tensor for Knowledge base completion is not satisfied in many practical cases (e.g., while (Washington, capital-of, USA) holds, (USA, capital-of, Washington) does not). ComplEx proposes an alternative where the subject and object modes share the parameters of the factors, but are complex conjugates of each other. More precisely, this approach represents a real-valued tensor $\mathbb{X} \in \mathbf{R}^{N_1 \times N_2 \times N_3}$ as the real part of a sum of $R$ complex-valued rank one tensors $u_{r}^{(1)} \otimes u_{r}^{(2)} \otimes \bar{u_{r}}^{(1)} $ where $r \in \{1,\ldots, R\}$, and $u_{r}^{(m)} \in \mathbf{C}^{N_m}$
\begin{equation}
    \mathbb{X}=Re(\sum\limits_{r=1}^{R}u_{r}^{(1)} \otimes u_{r}^{(2)} \otimes \bar{u_{r}}^{(1)}),
\end{equation}
where $\bar{u_{r}}^{(1)}$ is the complex conjugate of $u_{r}^{(1)}$.\\
\textbf{Training}
Given a training triple $(i,j,k)$ and a predicted tensor $\mathbb{X}$, the instantaneous multi-class log-loss $l_{i,j,k}(\mathbb{X})$ is 
\begin{equation}
    l_{i,j,k}(\mathbb{X})=l_{i,j,k}^{(1)}(\mathbb{X})+l_{i,j,k}^{(3)}(\mathbb{X})
\end{equation}
\begin{equation*}
    l_{i,j,k}^{(1)}=-\mathbb{X}_{i,j,k} + log(\sum_{k^{'}} exp(\mathbb{X_{i,j,k^{'}}})) 
\end{equation*}
\begin{equation*}
    l_{i,j,k}^{(3)}=-\mathbb{X}_{i,j,k} + log(\sum_{i^{'}} exp(\mathbb{X_{i^{'},j,k}}))
\end{equation*}
The final tensor is computed by finding a minimizer of a regularized empirical risk formulation, where the factors $u_{r}^{(d)}$ are weighted in a data-dependent manner by $w_{S}^{(d)}$, as:
\begin{equation}
    \min_{\substack{(u_r^{(d)}) \\ {d=1,\ldots,3 \\ r=1,\ldots,R}}} \sum\limits_{(i,j,k) \in S} l_{i,j,k}(\sum\limits_{r=1}^{R} u_{r}^{(1)} \otimes u_{r}^{(2)} \otimes u_{r}^{(3)}) + \lambda\sum\limits_{r=1}^{R}\sum\limits_{d=1}^{3}\norm{w_{S}^{(d)}\odot u_{r}^{(d)}}^{2},
\end{equation}
where $\odot$ is the entry-wise multiplication of vectors.




%uoooooooooooooooo tumadreuooooooooooooooooooo UOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO
%AL FIN SE TERMINA ESTA PUTA MIERDA!!!!
%USEGREAS OSTOJEOGIRN ojeogiek


\end{document}
