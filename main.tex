\documentclass[11pt, english]{article}
\usepackage{graphicx}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}
\usepackage[english]{babel}
\selectlanguage{english}
\usepackage[utf8]{inputenc}
\usepackage[svgnames]{xcolor}
\usepackage{amsmath}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator*{\E}{\mathbb{E}}



\usepackage{listings}
\usepackage{afterpage}
\pagestyle{plain}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

%\lstset{language=R,
%    basicstyle=\small\ttfamily,
%   stringstyle=\color{DarkGreen},
%    otherkeywords={0,1,2,3,4,5,6,7,8,9},
%    morekeywords={TRUE,FALSE},
%    deletekeywords={data,frame,length,as,character},
%    keywordstyle=\color{blue},
%    commentstyle=\color{DarkGreen},
%}

\lstset{frame=tb,
language=R,
aboveskip=3mm,
belowskip=3mm,
showstringspaces=false,
columns=flexible,
numbers=none,
keywordstyle=\color{blue},
numberstyle=\tiny\color{gray},
commentstyle=\color{dkgreen},
stringstyle=\color{mauve},
breaklines=true,
breakatwhitespace=true,
tabsize=3
}

\usepackage{here}


\textheight=21cm
\textwidth=17cm
%\topmargin=-1cm
\oddsidemargin=0cm
\parindent=0mm
\pagestyle{plain}

%%%%%%%%%%%%%%%%%%%%%%%%%%
% La siguiente instrucciÃ³n pone el curso automÃ¡ticamente%
%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{color}
\usepackage{ragged2e}

\global\let\date\relax
\newcounter{unomenos}
\setcounter{unomenos}{\number\year}
\addtocounter{unomenos}{-1}
\stepcounter{unomenos}
\gdef\@date{ Course \arabic{unomenos}/ 2019}

\begin{document}

\begin{titlepage}

\begin{center}
\vspace*{-1in}
\begin{figure}[htb]
\begin{center}
\includegraphics[width=8cm]{logo}
\end{center}
\end{figure}

INFORMATION \& NETWORK DYNAMICS (INDY) LAB - \@date\\
\vspace*{0.4in}
\begin{large}
MATRIX FACTORISATION\\
\end{large}
\vspace*{0.2in}
\begin{Large}
\textbf{Optimal Regularizer} \\
\end{Large}
\vspace*{0.3in}
\begin{large}
Ritabrata Ray \\
\end{large}
\vspace*{0.3in}
\rule{80mm}{0.1mm}\\
\vspace*{0.1in}
\begin{large}
IIT Kharapur \\
\end{large}
\includegraphics[width=2cm]{LogoFac.jpg}
\end{center}
\end{titlepage}

\newcommand{\CC}{C\nolinebreak\hspace{-.05em}\raisebox{.4ex}{\tiny\bf +}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf +}}
\def\CC{{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}}

\tableofcontents
\newpage
\section{Objective}
In this work, we aim to find the optimal values of the regularization strength of different parameters with different frequencies for the famous Matrix Completion Problem for the Netflix Problem. The parameters here are the regularization strengths of each embedding vector for each user and each movie.  
%Our Optimisation problem is as follows : \\
%\begin{center}
%\min\limits_{x \in R^d} f(x) & = \[ \frac{1}{n} %\sum\limits_{i=1}^{n} f \limits_{i}(x) \]
%\end{center} 
%\\ Convex function definition & L-lipchitz smooth  
\section{Matrix Factorization for the Netflix Problem}
The Netflix Problem involves a matrix $R$ of dimension $m \times n$ corresponding to $m$ users and $n$ items(movies). The $(i,j)^{th}$ entry of $R$ is a non-negative real number in $[0,5]$ is a rating of the $j^{th}$ item by the $i^{th}$ user. But the matrix $R$ is incomplete and on the basis of the available entries, we wish to predict the unavailable ones. We use latent factor model for this problem, which characterises each user and each item by a k-length \textit{embedding} vector. Where the inner product between the embedding vectors of the $i^{th}$ user and the $j^{th}$ item would be an approximation for the $(i,j)^th$ entry of $R$. We approximate $R$ as
\begin{equation}
    R\approx \hat{R}:=PQ 
\end{equation}
where $P$ is a $m \times k$ matix with each row being the embedding vector for each user and $Q$ is a $k \times n$ matrix with each column being the embedding vector for each item.
Learning the matrices $P$ and $Q$ from the observed data would make us predict $\hat{R}$ which is a good approximation to R.\\
Thus, we have
\begin{equation}
    \hat{r}_{ij}=\vec{p_{i}}^{T}\vec{q_{j}}=\sum\limits_{k=1}^{K}p_{ik}q_{kj}
\end{equation} 
Now, we attempt to learn the matrices $P$ and $Q$ by minimizing the RMSE(root mean square error) between $R$ and $\hat{R}$ at the available entries of $R$, using Stochastic Gradient Descent. As the problem involves a large number of parameters to be learnt, $mk+kn$ to be precise, in order to avoid overfitting we use regularization by including the Frobenius norm of the two unknown matrices $P$ and $Q$ in the cost function.\\\\
Let $T$ be the set of available entries in the matrix i.e.,
\begin{equation}
    T:=\{(i,j)|r_{ij}~is~available~in~R\}
\end{equation}
 %So, the total cost function is:
% \begin{equation}
    % L:=\sum\limits_{(i,j)\in T} (r_{ij}-\vec{p_{i}}\cdot 5\vec{q_{j}})^{2} +\norm{P}^2+\norm{Q}^2
 %\end{equation}
 If we uniformly sample an entry $(i,j)$ from $T$, the instantaneous cost function due to this entry would be:
 \begin{equation}
     \hat{l}=(r_{ij}-\vec{p_{i}}\cdot\vec{q_{j}})^2 + \norm{\vec{p_{i}}}^2+\norm{\vec{q_{j}}}^2
 \end{equation}
 where the norms are the $L_2$ norms of the vectors and we have the net cost function as:
 
 \begin{equation}
    L:= \mathbb{E}[\hat{l}]=\frac{1}{|T|}\sum\limits_{(i,j)\in T}(r_{ij}-\vec{p_{i}}\cdot\vec{q_{j}})^2+\frac{1}{|T|}\sum\limits_{i=1}^{m} s_i\norm{\vec{p_i}}^2+\frac{1}{|T|}\sum\limits_{j=1}^{n} t_j\norm{\vec{q_{j}}}^2
 \end{equation}
 where
 \begin{align}
      s_i &:=  |\{j|r_{ij}\in T\}| \\
      t_j  &:=  |\{i|r_{ij} \in T\}|
 \end{align}
 and 
 \begin{align}
     \sum\limits_{i=1}^{m} s_i &= |T|  \\
     \sum\limits_{j=1}^{n} t_j &= |T|
 \end{align}

\section{Knowledge Base Completion using Tensor Decomposition}





%uoooooooooooooooo tumadreuooooooooooooooooooo UOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO
%AL FIN SE TERMINA ESTA PUTA MIERDA!!!!
%USEGREAS OSTOJEOGIRN ojeogiek


\end{document}
